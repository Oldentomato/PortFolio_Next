---
title: 뉴럴 네트워크 디자인 팁
date: 2023-06-16
description: 뉴럴 네트워크의 디자인과 최적화에 대한 설명
tag: [딥러닝]
---  

가장 최적의 뉴럴 네트워크를 디자인할 수 있는 여러가지 방법들에 대한 게시글을 가져온 내용들이다.  
[참고한 블로그](https://junklee.tistory.com/24)  
  
## 1. 기본적인 뉴럴 네트워크 구조  
![DL_1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwS79K%2FbtqCM8NhHZz%2FqwxsxUItj9QoWkDxGjkW00%2Fimg.png)  
### Input neurons
- Input neurons의 수는 결국 내가 사용하고자 하는 Feature의 수와 동일하다.  
- MNIST 데이터셋의 경우, 28x28 이미지를 Flatten시켜 사용하기에, Feature의 수는 784개가 되고, Input neurons의 수 또한 784개가 되어야 한다.  
  
![DL_2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FOi5Sx%2FbtqCLL569zg%2FLjw8kbgJxF1hsGny1g6iS1%2Fimg.png)  
### Output neurons  
- Output neurons의 수는, 내가 예측하고자 하는 경우의 수이다.  
- 회귀(Regression): 회귀의 경우, Output은 단일값이다.  
- 분류(Classification): Binary classification(스팸 메일 구분 등)의 경우, 우리는 Output을 단일값으로 처리한다. 이 때 단일값은 0 과 1 사이의 값으로 확률이 해설될 수 있으며, 1=True, 0=False로 해석한다.  
Multi-class classification(Object detection 등)의 경우, 벡터로 처리할 수 있다. [0.2, 0.1, 0.7]과 같은 Output은 2번째 Class에 속할 확률이 70%라 해석될 수 있으며, Softmax등의 activation을 통해 처리한다.  

### Hidden Layers and Neurons per Hidden Layers  
- Hidden layers의 수는, 좋은 Neural network를 구성하는 중요 요소이다. 너무 크지도, 너무 작지도, 정말 적당하게 만들어야 한다.  
- 일반적으로, 1~5개의 Hidden layer로 대부분의 문제를 잘 해결할 수 있다. 물론 이미지 또는 언어 데이터를 다루고자 한다면, 몇 백개의 layer가 필요할 것이며, 이때 모든 레이어가 Fully connected여서는 안된다.  
이런 용도로는 pre-trained model인 Yolo, ResNet, VGG를 기반으로 약간의 layer를 얹어 데이터를 추가학습하는 것으로 훨씬 빠르고, 좋은 결과를 얻어낼 수 있다.  
- 일반적으로, 모든 Hidden layer에 같은 수의 neurons을 사용하는 건 유효하며, 데이터에 따라 많은 수의 neurons에서 적은 수의 neurons로 layer의 폭을 줄여나가는 방법을 택할 수 있다. 이렇게하면, 첫번째 layer에서는 low level의 feature를 다루고,
마지막 layer에서는 high level의 feature를 처리하게 된다.  
![DL_3](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwuyVD%2FbtqCID2q9W3%2Fn2TQ8ksvh0o3ybnkJXeyg0%2Fimg.jpg)  
- 보통 Neural network의 폭을 늘리는 것보다는 깊이를 쌓는 것이 정확도 개선에 효과적이다. (보통 width와 depth의 균형을 잡으라고 표현한다만, 대개의 layer가 50개 이상의 neurons을 갖는 경우가 많아서 이렇게 표현한 것으로 보인다.)  
- 처음에는 1~5개의 layer와 1~100개의 neurons에서 시작해 천천히 더 많은 layer와 neurons를 추가해나가는 것을 추천한다.  
- 만약 layer/neurons의 수가 너무 작다면, 현재 network는 데이터로부터 근본적인 패턴을 학습하는 것에 실패할 것이다. 이를 방지하기 위해 많은 수의 layer/neurons를 갖는 network를 구성한 다음, dropout 또는 early stopping 등을 활용하여 적절한 사이즈를 찾아나가야 한다.  
- Andrej Karpathy 또한 다음과 같은 말로 과적합 후 정규화 접근 방식에 대해 추천한다. "일단은 학습데이터에 충분히 오버피팅되는 모델을 구성한 뒤(training loss에만 집중함으로써), 정규화를 적절히 사용하여 validation loss를 줄여나가십시오."  
### Loss function  
![DL_4](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbolIDa%2FbtqCGRtqdok%2Fi9Lz7zBaJNaSKkgr9F9Df0%2Fimg.png)  
- 회귀(Regression): MSE가 가장 일반적이며, 이상치가 많을 경우에는 MAE 또는 Hubber loss를 사용할 수 있다.  
- 분류(Classification): Cross-entropy loss function은 대부분의 분류문제에서 잘 작동한다.  

### Batch Size  
- 큰 배치 사이즈는 같은 시간에 더 훌륭한 결과를 얻어낼 수 있게 한다. 이미지 분류 또는 언어 모델링의 경우 기본적으로 데이터가 굉장히 크기에, 좋은 효과를 얻어낼 수 있다.  
- 그러나 batch size가 커질수록, 알맞는 learning rate의 번위가 좁아진다. 때문에 데이터와 모델마다 적절한 batch size와 learning rate를 찾아나가야 한다. 보통 최고의 performance는 2~32 batch size에서 얻어진다. 특히나 다루는 데이터가 
굉장히 거대하지 않다면, lower batch size에서 천천히 값을 키워나가며 performance를 monitoring하는 과정이 필요하다.  
### Number of epochs  
- 많은 수의 Epochs에서 시작하여 early stopping을 통해 조율하는 것을 추천한다. (train_loss는 낮아지는데 val_loss는 증가하기 시작하는 시점에서 학습을 중지)
### Scaling yout features  
![DL_5](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F5JQzQ%2FbtqCIENV3my%2FN7k2Ym12ZkHpw9ZQmom5w1%2Fimg.png)  
- 간단히, Feature를 scaling하면 학습 개선에 효과적이다. 특히 모델 학습 과정에서 Batchnormalization을 적용하는 과정은 효과가 좋다.  
  
## 2. 학습률(Learning Rate)  
![DL_6](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F3fG3u%2FbtqCLdIx3ED%2FOa7hUmCkkGeGs0kGX7MIyK%2Fimg.png)  
- 적절한 learning rate를 찾는 것이 굉장히 중요하다. network의 hyper-parameters를 조절할 때마다, learning rate도 같이 조절되어야 한다.  
- 최고의 learning rate를 찾기 위해서 매우 낮은 값(10^-6)에서부터 시작해서 10을 곱하면서 키워나간다. learning rate별 loss를 확인하며 최적의 값을 찾아내야 한다.  
  
## 3. 관성(Momentum)기법  
![DL_7](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fl32Fe%2FbtqCJltGGRC%2F2DWaHfMKlylyoslcSNFDYk%2Fimg.png)  
- 일반적으로 SGD는 모든 STEP을 밟아나가기에 많고 오래걸리는 연산이 필요하다. 이 때, Momentum을 활용하면, local minima를 벗어나고, 학습 속도를 개선할 수 있다.  
- 일반적으로 momentum값은 1과 가깝다. 작은 데이터셋이라면 0.9가 적당하며, 큰 데이터셋이라면 0.999를 사용해볼만하다. 사실 이러한 옵션들을 활용한 다양한 optimizer가 존재한다.  

## 4. 기울기 소실, 폭발(Vanishing + Exploding Gradients)  
![DL_8](https://blog.kakaocdn.net/dn/phlHR/btqCID2q91P/6odTKczBWDoS9C8genfuKk/img.gif)  
- Neural network의 layers는 같은 정도로 학습되지 않는다. Output layer로부터 발생된 error gradient는 역전파됨에따라 값이 점점 작아지고, 첫번째 layer에 도착했을 때는 값이 굉장히 작아진다. 때문에 첫번째 layer는 거의 학습되지 않는다.  
- 이것은 Vanishing gradient문제이다. (반대로 gradient가 과도하게 큰 문제를 exploding gradient라 말한다. 이 경우, 어느 한 레이어에 의해 output이 크게 좌우될 것이다.)  
- 아래는 이들의 해결법들이다.  

### 활성 함수(Activation functions)  
- Hidden Layer Activation  
  - 일반적으로, logistic -> tanh -> Leaky ReLU -> ELU -> SELU 순으로 성능이 개선된다. 다만, 당연히 각각에 따른 연관 parameter 수정은 필요하다.  
  ReLU는 가장 인기있는 Activation function이다. 별 생각이 없다면 ReLU로 시작해도 무방하다. 그러나 ELU와 SELU를 통한 모델 개선이 가능함을 명심해야 한다.  
  
    - overfitting으로 고민한다면: RReLU  
    - runtime latenct를 줄이고자 한다면: leaky ReLU  
    - 데이터셋이 거대하다면: PReLU  
    - 빠른 추론시간이 필요하다면: leaky ReLU  
    - network가 자가적으로 정규화(normalization)를 시행할 수 없다면: ELU  
    - robust한 activation이 필요하다면: SELU  
    
### Output Layer Activation  
- 회귀(Regression): 회귀 문제는 대개 activation function을 지정할 필요가 없다. 1 node를 갖는 dense layer에서 알아서 단일값으로 뱉어주기 때문이다.  
만약 우리가 출력값을 특정 범위로 국한시키고 싶을 때는, tanh나 logistic function을 활용할 수 있다.  
- 분류(classification): Binary cassification의 경우 sigmoid를 통해 0~1값을 출력하고, Multi-class classification의 경우 softmax를 사용하는 것이 일반적이다.  

### Weight initialization method  
- 올바른 weight initialization method를 택하는 것은 수렴에 걸리는 시간을 높이기 위해 중요하며 activation에 따라 선택되어야한다.  
- ReLU or leaky ReLU 라면 He initialization  
- SELU or ELU라면 LeCun initialization  
- softmax, logistic, or tanh라면 Glorot initialization  
- 대부분의 init method는 uniform and normal distribution의 특징을 가진다.  

### BatchNormalization  
- BatchNorm은 단순히, 각 layer의 inputs을 normalizing하는 것으로써, 모델이 평균과 스케일의 최적값을 배울 수 있게 한다. 이를 사용할 경우 dropout, l2 정규화 등을 사용할 필요가 없다.  
- BarchNorm을 사용하면 더 높은 Learning rate를 사용해볼 수 있다.  
- 한가지 단점은 각 계층마다 normalization을 진행하는 시간이 추가로 소요된다는 것이다.  

### Early Stopping  
- train loss는 계속해서 낮아지는데 valid loss는 증가하는 경우, train set에 대한 학습을 종료하여 과대적합을 피하는 것이 나은 선택일 수 있다.  


## 5. Dropout  
- Dropout은 간단히 사용할 수 있으면서도 모델의 개선을 이끌어낼 수 있는 좋은 테크닉이다. 이는 학습 도중 일부 노드의 값을 0으로 만들어 버리는 것으로, Network를 Robust하게 만들 수 있다. 즉, 어떤 Neurons에 과의존하지 않는다.  
- Dropout의 비율은 0.1 ~ 0.5정도가 좋으며 RNN의 경우 0.3, CNN의 경우 0.5가 일반적이다. 거대한 layer일수록 비율을 늘려야 한다. Dropout의 증가는 Overfitting을 막으며, 작은 Dropout은 Underfitting을 방해한다.  
- Output layer에는 Dropout을 사용하지 않는다.  

## 6. Optimizers  
![DL_9](https://blog.kakaocdn.net/dn/wvSxe/btqCKhq7ng2/RBHQVvwUnRYp5sPnP2KXDK/img.gif)  
- 수렴의 질에 관심이 있고, 그에 대한 소요시간에 대한 걱정이 없다면, SGD를 사용하면 된다.  
- 소요시간을 줄어야하고, 적당한 최적값을 원한다면, Adam, Nadam, RMSprop, Adamax optimizers를 사용해보면 좋다.  
- Andrej Karpatthy의 말로는 CNN의 경우 SGD를 잘 튜닝하는 것으로 Adam의 성능을 따라잡을 수 있다고 한다.  

## 7. Learning Rate Scheduling  
![DL_10](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FxU2mP%2FbtqCLdIx3Ho%2FGiPiZHv4Isknmt6LWNCbu0%2Fimg.png)  
- 우리는 이미 learning rate의 중요성에 대해 언급했다. 하지만 이것이 너무 높기를 원하지는 않는다. 이 경우, Loss function은 최적해 근처에서 수렴하지 못하고 되고 너무 낮을 경우, 수렴에 오랜 시간이 걸리게 될 것이다. 
- 처음에는 높은 학습률을 설정하여 빠르게 학습하고, 이후에는 낮은 학습률을 설정하여 면밀히 탐색하는 Learning rate scheduling을 사용할 수 있다.  
- Learning rate를 점진적으로 감소시켜나가는 기법 외에도 Step function을 사용하거나, Performance가 변화하는 것을 보고 값을 수정하는 방법도 가능하다. 앞선 커널에서는, ReduceLROnPlateau callback을 사용하여, Performanc가 하락함에 따라 Learning rate를 줄여나가는 방법을 사용했다.  
- 1-cycle scheduling을 사용하는 것도 추천한다. 
- 다른 Hyper parameters를 찾을 때 까지는 고정된 Learning rate를 사용하고, 마지막에 Learning rate decay scheduling을 사용해야 한다.  

## 8. A Few More Things  
- 효율적인 방법으로 Network를 Scaling하고 싶다면 다음 논문을 참고하면 좋다. [EfficientNets](https://arxiv.org/pdf/1905.11946.pdf)  
- 위 논문을 읽으면 learning rate, batch sizes, momentum and weight decay techniques 에 대한 추가적인 내용을 배울 수 있다.  
- 그리고 Stochastic Weight Averagin에 대한 [이 글](https://arxiv.org/abs/1803.05407)은 더 좋은 일반화를 보여준다.  
- [Andrej Karpathy's excellent guide](http://karpathy.github.io/2019/04/25/recipe/)를 읽으면 Network를 구성하는 더 많은 방법을 학습할 수 있다.  









