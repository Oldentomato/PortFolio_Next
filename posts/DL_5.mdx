---
title: 딥러닝 개요
date: 2023-04-24
description: 유튜브 코딩애플의 딥러닝 강의를 풀어쓴 내용
tag: [딥러닝, 다층 퍼셉트론]
---  

# 딥러닝에 대하여
머신러닝의 일부중 하나로 기존 머신러닝에서는 이미지에 인식을 시킬 때 기본적인 특징들을 알려줘야 하지만,  
딥러닝은 그런 방식을 쓰지 않아도 스스로 특징들을 분석하여 인식을 한다.  
## 종류
- 지도학습  
- 비지도학습  
- 강화학습  

![DL_1](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_1.png?raw=true)  

오차를 줄이기 위해서 가중치들을 계속해서 바꿔가며 최소의 오차를 구하는 것.  
즉, 최적의 가중치를 찾는 것이 머신러닝의 역할이다.  
편향은 Input에는 관련이 없지만, Output에 관련이 있는 요소들이 있을 경우 계산할 때 쓰인다.  
필요할때만 쓰인다. (가중치와 편향을 찾는다)  
![DL_2](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_2.png?raw=true)  
hidden layer에는 단순히 숫자정보들만 있다. 이러한 숫자들은 계산에 의해 들어가는데 계산법은 앞서말한 퍼셉트론대로 하면 된다.  
![DL_3](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_3.png?raw=true)  
  
h1 = 6월 * w1 + 9월 * w2
h2 = 6월 * w3 + 9월 * w4  
  
## 손실함수  
손실함수는 오차를 구할 때 쓰는 함수로 구하는 방법은 예측값과 실제값의 차이 (절대값)들을 구하고 각각 제곱한 뒤 그 숫자들의 평균값을 구하면 된다.  
수식으로는 $$\frac{1}{n}  \sum_.^.( \hat{y} - y)^{2}$$ 이다.  $\hat{y}$가 실제값이고 y가 예측값이다.  
하지만 실전에서는 위 수식보다 더 복잡한 수식의 함수가 지원되어있기 때문에 상황에 맞는 손실함수를 써주면 된다.  
  
현재 저대로의 방식대로 하면 큰 결함이 하나 있다.  
예시로 설명하자면 위의 그림을 토대로 식을 쓰게 되면  
h1w5 + h2w6 = (6w1 + 9w2)w5 + (6w3 + 9w4)w6  
이렇게 되는데 여기서 식을 풀고 정리하게 되면  
6w1w5 + 9w2w5 + 6w3w6 + 9w4w6  
= 6(w1w5 + w3w6) + 9(w2w6 + w4w6)  
= 6z1 + 9z2  
이렇게 표현이 된다.  
히든레이어 없는 식 = 6w1 + 9w2  
히든레이어 있는 식 = 6z1 + 9z2  
히든 레이어가 있어도 없을 때와 거의 같기 때문에 문제가 발생한다.  
이를 해결해주는 것이 활성화 함수이다.  
  
## 활성화 함수  
활성화 함수는 계산이 된 히든레이어의 숫자를 정해진 수식대로 계산하여 값을 변경시킨다.  
그 중에서 가장 유명한 것이 시그모이드(sigmoid) 함수이다.  
수식은 $\frac{e^x}{e^x + 1}$ 이고 x에 수를 대입하면 된다.  
결과값은 0~1 사이의 숫자만 나오게 되어있다. (시그모이드 기준)  
![DL_4](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_6.png?raw=true)  
그 밖에도 tanh, relu 등 여러 함수들이 있는데 상황에 맞게 써주면 된다.  
- 활성화 함수없이 예측: 선형적이고 단순한 예측  
- 활성화 함수 포함한 예측: 비선형적이고 복잡한 에측 가능  
  
## 비용함수  
이제 마지막으로 총손실(오차)를 최소화하는 w를 찾는법을 알아야 한다. 바로 경사하강법을 이용하면 된다.  
(예시)모든 가중치를 그릴수 없으니 예시로 하나만 이용한다.  
![DL_5](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_5.png?raw=true)  
처음에는 아무지점(점: 랜덤)에 지정을 한 뒤, 그 지점의 접선의 기울기를 구한다.  
그리고 현재 지점에서 기울기만큼 빼주면 3 - 1 = 2 가 된다.  
말그대로 경사로 인해 하강하는 것처럼 보여 경사하강법이라 한다.  
참고로 기울기는 / 방향이면 양수, \ 방향이면 음수이다.  
수식으로 표현하면 $$\omega1'= \omega1 - \alpha \frac{\partial \* E}{\partial \* \omega1}$$ 
해당수식에서 w1이 총 손실 E에 얼마나 큰 영향을 끼치는가. 즉 기울기이다.(현재 가중치 하나의 기준으로)  
  
따라서 딥러닝 학습과정은  
1. w값을 랜덤으로 찍음
2. w값을 바탕으로 총손실 E를 계산함  
3 경사하강법으로 새로운 w값 업데이트  
4. 2,3번을 반복 ...(총손실 E가 더이상 안줄어들 때 까지)  
  
그러나 여기에는 문제점이 하나 있다.  
![DL_6](https://github.com/Oldentomato/PortFolio_Next/blob/main/postsimg/DL_5/iamge_4.png?raw=true)  
만일 위 그림의 동그란 점이 랜덤으로 찍히고 기울기가 0까지 다다르게 되면 최저의 오차값을 찾지못하고 종료하게 될 것이다.  
이것의 해결책은 기울기만 빼지말고 learning rate * 기울기 만큼 빼면된다.  
$\alpha \frac{\partial \* E}{\partial \* \omega1}$ 에서 $\alpha$가 learning rate이다.  
만일 기울기가 1이고 learning rate가 3이면 -3 이 계산이 되므로 지점은 네모점으로 된다. 기본적으로 0.01로 시작하여 조정해 나가면 된다.  
  
하지만 고정된 값만 주면 복잡한 문제일 경우 학습이 안일어날 수도 있다.  
그래서 함수를 이용하여 가변적으로 변하게 할 수 있다.  
그것이 optimizer이다. 종류로는  
- Momentum: 가속도를 유지  
- AdaGrad: 자주 변하는 w는 작게, 자주변하면 크게  
- RMSprop: AdaGrad인데 제곱을 함  
- AdaDelta: AdaGrad인데 a(알파)가 너무 작아져서 학습이 안되는거 방지함  
- Adam: RMSprop + Momentum  
대체로 Adam을 가장 많이 사용한다.  